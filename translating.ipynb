{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ae1bdd5-ea36-4958-873f-e950083d1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93034810-b4f0-4b6f-a6a9-9b81fb6ceee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dca5a26890442bbc173c6d2b21c85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, NllbTokenizerFast\n",
    "model_name=\"facebook/nllb-200-3.3B\"\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = NllbTokenizerFast.from_pretrained(model_name,tgt_lang=\"heb_Hebr\",src_lang=\"eng_Latn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5fbce51-daef-4864-9369-6933b037f412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): Embedding(256206, 2048, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): Embedding(256206, 2048, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): Embedding(256206, 2048, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256206, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1ed520e-db42-4924-ba55-e6a5c090bdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מה שלומך?\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def translate_text(text):\n",
    "    # Tokenize and translate the text\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "    #manual fix to hf bug \n",
    "    encoded_text['input_ids'][:,1]=tokenizer.lang_code_to_id[tokenizer.src_lang]\n",
    "\n",
    "    encoded_text={k:v.to(model.device) for k,v in encoded_text.items()}\n",
    "    generated_tokens = model.generate(**encoded_text,forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang]).cpu()\n",
    "\n",
    "    # Decode and return the translated text\n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "\n",
    "translated_text = translate_text(text_to_translate)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "439cacc3-a447-4a6b-b738-be0ea7e79bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"מה היו הטענות הספציפיות של אבישי גרינזאג, צ'אן מאניט, ופרסום גלובס בנוגע לבקשת צילום, הקלטה והפצת ההליכים הפליליים נגד בנימין נתניהו בפני בית המשפט המחוזי בירושלים, ומה הייתה החלטת בית המשפט העליון בנוגע לבקשת זו?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text(\"\"\" What were the specific allegations made by Avishai Greenzayg, Che\n",
    "n Maanit, and Globes Publisher regarding the request to film, record, and broadc\n",
    "ast the criminal proceedings against Benjamin Netanyahu before the Jerusalem Dis\n",
    "trict Court, and what was the decision of the Supreme Court regarding this reque\n",
    "st?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60f2b537-e552-48ed-be06-b3b508888eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'האם החברה \"צ\\'ופן אבטחה שמירה וניקיה יון\" העלתה סוגיות ספציפיות בנוגע לתקנות קבלת רישיון לשירותי אבטחה, ואיך פסק בית המשפט על עתירתם?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text(\"\"\"What specific issues did the company \"Tzofen Avtaha Shmira V'niki\n",
    "yon\" raise regarding the regulations for obtaining a license for security servic\n",
    "es, and how did the court rule on their petition?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f674e61f-72e9-4531-a430-15841fb37968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'האם ישנן מועדים ספציפיים להגיש את סיכום התביעות וההתגובות בכתב, ואת הטיעון הפנימי בתיק המעורב ב\"חברת הביטוח קלל\" ו\"מנהל המתקנים הגדולים\"?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text(\"\"\"What are the specific deadlines for submitting the written summar\n",
    "ies of claims and responses, and the oral argument in the case involving \"Klal I\n",
    "nsurance Company\" and the \"Manager of Large Facilities\"?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "398c8663-8df6-4b9a-b702-2678a4375879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng_Latn'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.src_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2182496a-fc19-42ad-a6bc-c2e7c0c5db99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heb_Hebr'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tgt_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cc8d324-f93b-4c46-8d01-dbd6f63f0560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256047"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.lang_code_to_id[tokenizer.src_lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb426ea2-1353-43c6-bd77-25956e83cbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(256184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "316735d8-ab0a-4cde-bfd6-2c82dd3aee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng_Latn'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.src_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1131dbd1-fe38-4498-97b5-6710b229574b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NllbTokenizerFast' object has no attribute '_start_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_token_id\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NllbTokenizerFast' object has no attribute '_start_token_id'"
     ]
    }
   ],
   "source": [
    "tokenizer._start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158e95f-986e-4395-a276-709156cb6524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
